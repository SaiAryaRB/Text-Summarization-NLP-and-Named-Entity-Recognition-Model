{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b58787fe",
      "metadata": {
        "id": "b58787fe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BartTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"gopalkalpande/bbc-news-summary\")\n",
        "train_data = dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6f83d0a",
      "metadata": {
        "id": "b6f83d0a"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Define a custom Dataset class\n",
        "class TextSummarizationDataset(Dataset):\n",
        "    def __init__(self, data, max_input_length=100, max_target_length=30):\n",
        "        self.data = data\n",
        "        self.max_input_length = max_input_length\n",
        "        self.max_target_length = max_target_length\n",
        "        self.input_texts = []\n",
        "        self.target_texts = []\n",
        "        self.build_dataset()\n",
        "\n",
        "    def build_dataset(self):\n",
        "        for example in self.data:\n",
        "            input_text = example[\"Articles\"]\n",
        "            target_text = example[\"Summaries\"]\n",
        "            # Truncate or pad input text to max_input_length\n",
        "            input_text = input_text[:self.max_input_length]\n",
        "            input_text = input_text + ' ' * (self.max_input_length - len(input_text))\n",
        "            # Truncate target text to max_target_length\n",
        "            target_text = target_text[:self.max_target_length]\n",
        "            self.input_texts.append(input_text)\n",
        "            self.target_texts.append(target_text)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\"input_text\": self.input_texts[idx], \"target_text\": self.target_texts[idx]}\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_text(text):\n",
        "    tokens = text.split()\n",
        "    token_ids = [vocab_to_idx[token] if token in vocab_to_idx else vocab_to_idx['<UNK>'] for token in tokens]\n",
        "    return token_ids\n",
        "\n",
        "# Build vocabulary\n",
        "all_texts = [example[\"Articles\"] for example in train_data] + [example[\"Summaries\"] for example in train_data]\n",
        "all_tokens = ' '.join(all_texts).split()\n",
        "vocab_counter = Counter(all_tokens)\n",
        "vocab = [token for token, count in vocab_counter.items() if count > 5]  # Filter out tokens with count <= 5\n",
        "vocab_to_idx = {token: idx for idx, token in enumerate(vocab)}\n",
        "vocab_to_idx['<PAD>'] = len(vocab_to_idx)\n",
        "vocab_to_idx['<UNK>'] = len(vocab_to_idx)\n",
        "\n",
        "# Define collate function for DataLoader\n",
        "def collate_fn(batch):\n",
        "    input_texts = [item[\"input_text\"] for item in batch]\n",
        "    target_texts = [item[\"target_text\"] for item in batch]\n",
        "    input_token_ids = [tokenize_text(text) for text in input_texts]\n",
        "    target_token_ids = [tokenize_text(text) for text in target_texts]\n",
        "\n",
        "    # Pad sequences to maximum length\n",
        "    input_tensor = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(ids) for ids in input_token_ids], batch_first=True, padding_value=vocab_to_idx['<PAD>'])\n",
        "    target_tensor = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(ids) for ids in target_token_ids], batch_first=True, padding_value=vocab_to_idx['<PAD>'])\n",
        "\n",
        "    return {\"input_tensor\": input_tensor, \"target_tensor\": target_tensor}\n",
        "\n",
        "\n",
        "# Create dataset instance\n",
        "max_input_length = 100\n",
        "max_target_length = 30\n",
        "custom_dataset = TextSummarizationDataset(train_data, max_input_length, max_target_length)\n",
        "\n",
        "# Create DataLoader\n",
        "BATCH_SIZE = 32\n",
        "dataloader = DataLoader(custom_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Example usage\n",
        "for batch in dataloader:\n",
        "    input_tensor = batch[\"input_tensor\"]  # Shape: (batch_size, max_input_length)\n",
        "    target_tensor = batch[\"target_tensor\"]  # Shape: (batch_size, max_target_length)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67ef04a5",
      "metadata": {
        "id": "67ef04a5",
        "outputId": "4b130828-6cb5-4c52-fe6c-851b92c6d74c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input token length statistics:\n",
            "Mean: 15.759442446043165\n",
            "Max: 22\n",
            "Min: 11\n",
            "\n",
            "Target token length statistics:\n",
            "Mean: 5.60521582733813\n",
            "Max: 9\n",
            "Min: 3\n"
          ]
        }
      ],
      "source": [
        "# Print lengths of tokenized sequences for input and target texts\n",
        "input_token_lengths = [len(tokenize_text(text)) for text in custom_dataset.input_texts]\n",
        "target_token_lengths = [len(tokenize_text(text)) for text in custom_dataset.target_texts]\n",
        "\n",
        "print(\"Input token length statistics:\")\n",
        "print(\"Mean:\", np.mean(input_token_lengths))\n",
        "print(\"Max:\", np.max(input_token_lengths))\n",
        "print(\"Min:\", np.min(input_token_lengths))\n",
        "\n",
        "print(\"\\nTarget token length statistics:\")\n",
        "print(\"Mean:\", np.mean(target_token_lengths))\n",
        "print(\"Max:\", np.max(target_token_lengths))\n",
        "print(\"Min:\", np.min(target_token_lengths))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d8c023c",
      "metadata": {
        "id": "4d8c023c",
        "outputId": "ada00343-343d-4fba-8592-3c55aef0ef1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1:\n",
            "Input text: Budget to set scene for election..Gordon Brown will seek to put the economy at the centre of Labour'\n",
            "Tokenized input: [0, 1, 2, 3, 4, 15375, 5, 6, 7, 1, 8, 9, 10, 11, 9, 12, 13, 15375]\n",
            "Length of tokenized input: 18\n",
            "Target text: - Increase in the stamp duty t\n",
            "Tokenized target: [72, 15375, 19, 9, 46, 44, 15375]\n",
            "Length of tokenized target: 7\n",
            "\n",
            "Sample 2:\n",
            "Input text: Army chiefs in regiments decision..Military chiefs are expected to meet to make a final decision on \n",
            "Tokenized input: [278, 279, 19, 280, 15375, 279, 139, 29, 1, 218, 1, 281, 16, 282, 283, 74]\n",
            "Length of tokenized input: 16\n",
            "Target text: \"They are very much not for th\n",
            "Tokenized target: [386, 139, 180, 101, 64, 4, 15375]\n",
            "Length of tokenized target: 7\n",
            "\n",
            "Sample 3:\n",
            "Input text: Howard denies split over ID cards..Michael Howard has denied his shadow cabinet was split over its d\n",
            "Tokenized input: [446, 447, 448, 237, 449, 15375, 446, 123, 450, 24, 143, 451, 357, 448, 237, 452, 15375]\n",
            "Length of tokenized input: 17\n",
            "Target text: Michael Howard has denied his \n",
            "Tokenized target: [566, 446, 123, 450, 24]\n",
            "Length of tokenized target: 5\n",
            "\n",
            "Sample 4:\n",
            "Input text: Observers to monitor UK election..Ministers will invite international observers to check the forthco\n",
            "Tokenized input: [15375, 1, 614, 117, 15375, 6, 615, 616, 617, 1, 618, 9, 15375]\n",
            "Length of tokenized input: 13\n",
            "Target text: The report said individual reg\n",
            "Tokenized target: [40, 629, 62, 720, 15375]\n",
            "Length of tokenized target: 5\n",
            "\n",
            "Sample 5:\n",
            "Input text: Kilroy names election seat target..Ex-chat show host Robert Kilroy-Silk is to contest the Derbyshire\n",
            "Tokenized input: [767, 768, 96, 769, 15375, 770, 771, 772, 773, 28, 1, 774, 9, 15375]\n",
            "Length of tokenized input: 14\n",
            "Target text: UKIP's leader, Roger Knapman, \n",
            "Tokenized target: [856, 878, 839, 879]\n",
            "Length of tokenized target: 4\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print tokenized sequences and their lengths before converting to tensors\n",
        "for idx in range(5):  # Print information for the first 5 samples\n",
        "    print(f\"Sample {idx + 1}:\")\n",
        "    print(\"Input text:\", custom_dataset.input_texts[idx])\n",
        "    print(\"Tokenized input:\", tokenize_text(custom_dataset.input_texts[idx]))\n",
        "    print(\"Length of tokenized input:\", len(tokenize_text(custom_dataset.input_texts[idx])))\n",
        "    print(\"Target text:\", custom_dataset.target_texts[idx])\n",
        "    print(\"Tokenized target:\", tokenize_text(custom_dataset.target_texts[idx]))\n",
        "    print(\"Length of tokenized target:\", len(tokenize_text(custom_dataset.target_texts[idx])))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd25b3d",
      "metadata": {
        "id": "5cd25b3d",
        "outputId": "a58923d3-5a35-4ea2-b307-8c861471061c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1:\n",
            "Input tensor shape: torch.Size([32, 19])\n",
            "Target tensor shape: torch.Size([32, 8])\n",
            "\n",
            "First sequence in the batch:\n",
            "Input sequence: tensor([  960,  4414,   345, 14948, 15375, 15375,   960,  1639,   687,    28,\n",
            "            1,  2410,     1,  3906,    16,   676,  3292, 15375, 15374])\n",
            "Target sequence: tensor([ 1823,  3276,    62,   367, 15375, 15374, 15374, 15374])\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "for batch_idx, batch in enumerate(dataloader):\n",
        "    input_tensor = batch[\"input_tensor\"]  # Shape: (batch_size, max_input_length)\n",
        "    target_tensor = batch[\"target_tensor\"]  # Shape: (batch_size, max_target_length)\n",
        "\n",
        "    print(f\"Batch {batch_idx + 1}:\")\n",
        "    print(\"Input tensor shape:\", input_tensor.shape)\n",
        "    print(\"Target tensor shape:\", target_tensor.shape)\n",
        "    print()\n",
        "\n",
        "    # Print the first sequence in the batch to verify padding\n",
        "    print(\"First sequence in the batch:\")\n",
        "    print(\"Input sequence:\", input_tensor[0])\n",
        "    print(\"Target sequence:\", target_tensor[0])\n",
        "\n",
        "    # Break after printing the first batch\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2dd9d8a",
      "metadata": {
        "id": "d2dd9d8a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        embedded = self.embedding(input_seq)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return hidden, cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ddb087f",
      "metadata": {
        "id": "2ddb087f"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, hidden_size, num_layers=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input_seq, hidden, cell):\n",
        "        input_seq = input_seq.unsqueeze(1)\n",
        "        embedded = self.embedding(input_seq)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        predictions = self.softmax(self.fc(outputs.squeeze(1)))\n",
        "        return predictions, hidden, cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13ede430",
      "metadata": {
        "id": "13ede430"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, teacher_forcing_ratio=0.5):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
        "\n",
        "    def forward(self, input_seq, target_seq):\n",
        "        batch_size = input_seq.size(0)\n",
        "        target_len = target_seq.size(1)\n",
        "        target_vocab_size = self.decoder.fc.out_features\n",
        "\n",
        "        # Initialize outputs and hidden states\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(input_seq.device)\n",
        "        hidden, cell = self.encoder(input_seq)\n",
        "\n",
        "        # First input to the decoder is the < SOS > token\n",
        "        input_seq = target_seq[:, 0]\n",
        "\n",
        "        # Iterate over sequence length\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder(input_seq, hidden, cell)\n",
        "            outputs[:, t] = output\n",
        "            # Determine whether to use teacher forcing\n",
        "            teacher_force = random.random() < self.teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input_seq = target_seq[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "963dd393",
      "metadata": {
        "id": "963dd393"
      },
      "outputs": [],
      "source": [
        "TEACHER_FORCING_RATIO = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ff26220",
      "metadata": {
        "id": "5ff26220"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "767a9930",
      "metadata": {
        "id": "767a9930"
      },
      "outputs": [],
      "source": [
        "# Define model\n",
        "encoder = Encoder(input_size=len(vocab_to_idx), hidden_size=256)\n",
        "decoder = Decoder(output_size=len(vocab_to_idx), hidden_size=256)\n",
        "model = Seq2Seq(encoder, decoder, teacher_forcing_ratio=TEACHER_FORCING_RATIO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9401b2ba",
      "metadata": {
        "id": "9401b2ba",
        "outputId": "7c36a522-464e-4d3a-9fe6-064f619ab674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 4.6748, Val Loss: 4.5465\n",
            "Epoch 2: Train Loss: 4.5780, Val Loss: 4.4261\n",
            "Epoch 3: Train Loss: 4.4682, Val Loss: 4.2985\n",
            "Epoch 4: Train Loss: 4.3348, Val Loss: 4.2328\n",
            "Epoch 5: Train Loss: 4.2071, Val Loss: 4.0916\n",
            "Epoch 6: Train Loss: 4.1352, Val Loss: 3.9374\n",
            "Epoch 7: Train Loss: 3.9319, Val Loss: 3.7868\n",
            "Epoch 8: Train Loss: 3.8751, Val Loss: 3.6800\n",
            "Epoch 9: Train Loss: 3.6972, Val Loss: 3.5617\n",
            "Epoch 10: Train Loss: 3.5951, Val Loss: 3.4500\n",
            "Epoch 11: Train Loss: 3.5166, Val Loss: 3.3046\n",
            "Epoch 12: Train Loss: 3.3930, Val Loss: 3.1709\n",
            "Epoch 13: Train Loss: 3.2231, Val Loss: 3.0589\n",
            "Epoch 14: Train Loss: 3.1668, Val Loss: 2.9687\n",
            "Epoch 15: Train Loss: 3.0355, Val Loss: 2.8253\n",
            "Epoch 16: Train Loss: 2.9112, Val Loss: 2.7632\n",
            "Epoch 17: Train Loss: 2.8092, Val Loss: 2.6206\n",
            "Epoch 18: Train Loss: 2.7055, Val Loss: 2.5034\n",
            "Epoch 19: Train Loss: 2.6156, Val Loss: 2.4438\n",
            "Epoch 20: Train Loss: 2.5570, Val Loss: 2.4008\n",
            "Epoch 21: Train Loss: 2.4717, Val Loss: 2.3072\n",
            "Epoch 22: Train Loss: 2.3476, Val Loss: 2.1811\n",
            "Epoch 23: Train Loss: 2.2391, Val Loss: 2.0711\n",
            "Epoch 24: Train Loss: 2.1716, Val Loss: 2.0367\n",
            "Epoch 25: Train Loss: 2.0933, Val Loss: 2.0170\n",
            "Epoch 26: Train Loss: 2.0410, Val Loss: 1.9174\n",
            "Epoch 27: Train Loss: 1.9839, Val Loss: 1.8431\n",
            "Epoch 28: Train Loss: 1.9418, Val Loss: 1.8038\n",
            "Epoch 29: Train Loss: 1.8763, Val Loss: 1.8139\n",
            "Epoch 30: Train Loss: 1.8275, Val Loss: 1.7286\n",
            "Epoch 31: Train Loss: 1.7606, Val Loss: 1.6811\n",
            "Epoch 32: Train Loss: 1.6847, Val Loss: 1.6432\n",
            "Epoch 33: Train Loss: 1.6583, Val Loss: 1.5775\n",
            "Epoch 34: Train Loss: 1.6338, Val Loss: 1.5560\n",
            "Epoch 35: Train Loss: 1.5789, Val Loss: 1.5143\n",
            "Epoch 36: Train Loss: 1.5489, Val Loss: 1.4901\n",
            "Epoch 37: Train Loss: 1.5356, Val Loss: 1.4741\n",
            "Epoch 38: Train Loss: 1.4844, Val Loss: 1.4373\n",
            "Epoch 39: Train Loss: 1.4481, Val Loss: 1.4174\n",
            "Epoch 40: Train Loss: 1.4422, Val Loss: 1.4112\n",
            "Epoch 41: Train Loss: 1.4226, Val Loss: 1.3896\n",
            "Epoch 42: Train Loss: 1.3968, Val Loss: 1.3743\n",
            "Epoch 43: Train Loss: 1.3850, Val Loss: 1.3596\n",
            "Epoch 44: Train Loss: 1.3683, Val Loss: 1.3622\n",
            "Epoch 45: Train Loss: 1.3472, Val Loss: 1.3466\n",
            "Epoch 46: Train Loss: 1.3471, Val Loss: 1.3449\n",
            "Epoch 47: Train Loss: 1.3402, Val Loss: 1.3330\n",
            "Epoch 48: Train Loss: 1.3382, Val Loss: 1.3185\n",
            "Epoch 49: Train Loss: 1.3238, Val Loss: 1.3290\n",
            "Epoch 50: Train Loss: 1.3264, Val Loss: 1.3176\n",
            "Epoch 51: Train Loss: 1.3118, Val Loss: 1.3120\n",
            "Epoch 52: Train Loss: 1.3162, Val Loss: 1.3070\n",
            "Epoch 53: Train Loss: 1.3109, Val Loss: 1.2926\n",
            "Epoch 54: Train Loss: 1.3210, Val Loss: 1.3005\n",
            "Epoch 55: Train Loss: 1.3113, Val Loss: 1.2959\n",
            "Epoch 56: Train Loss: 1.3049, Val Loss: 1.3042\n",
            "Epoch 57: Train Loss: 1.2954, Val Loss: 1.3105\n",
            "Epoch 58: Train Loss: 1.3026, Val Loss: 1.3176\n",
            "Epoch 59: Train Loss: 1.3078, Val Loss: 1.3053\n",
            "Epoch 60: Train Loss: 1.2980, Val Loss: 1.2903\n",
            "Epoch 61: Train Loss: 1.2994, Val Loss: 1.2995\n",
            "Epoch 62: Train Loss: 1.3059, Val Loss: 1.3076\n",
            "Epoch 63: Train Loss: 1.3006, Val Loss: 1.2856\n",
            "Epoch 64: Train Loss: 1.2958, Val Loss: 1.2956\n",
            "Epoch 65: Train Loss: 1.2963, Val Loss: 1.3054\n",
            "Epoch 66: Train Loss: 1.3011, Val Loss: 1.2933\n",
            "Epoch 67: Train Loss: 1.3059, Val Loss: 1.2963\n",
            "Epoch 68: Train Loss: 1.2990, Val Loss: 1.3090\n",
            "Epoch 69: Train Loss: 1.3098, Val Loss: 1.3074\n",
            "Epoch 70: Train Loss: 1.3099, Val Loss: 1.3149\n",
            "Epoch 71: Train Loss: 1.3123, Val Loss: 1.3116\n",
            "Epoch 72: Train Loss: 1.3198, Val Loss: 1.3259\n",
            "Epoch 73: Train Loss: 1.3077, Val Loss: 1.3087\n",
            "Epoch 74: Train Loss: 1.3178, Val Loss: 1.3015\n",
            "Epoch 75: Train Loss: 1.2939, Val Loss: 1.2908\n",
            "Epoch 76: Train Loss: 1.2960, Val Loss: 1.2863\n",
            "Epoch 77: Train Loss: 1.2966, Val Loss: 1.2786\n",
            "Epoch 78: Train Loss: 1.2868, Val Loss: 1.2880\n",
            "Epoch 79: Train Loss: 1.2954, Val Loss: 1.2822\n",
            "Epoch 80: Train Loss: 1.2793, Val Loss: 1.2944\n",
            "Epoch 81: Train Loss: 1.2845, Val Loss: 1.2863\n",
            "Epoch 82: Train Loss: 1.2867, Val Loss: 1.2914\n",
            "Epoch 83: Train Loss: 1.2889, Val Loss: 1.2931\n",
            "Epoch 84: Train Loss: 1.2858, Val Loss: 1.2905\n",
            "Epoch 85: Train Loss: 1.2864, Val Loss: 1.2904\n",
            "Epoch 86: Train Loss: 1.2913, Val Loss: 1.2903\n",
            "Epoch 87: Train Loss: 1.3028, Val Loss: 1.2853\n",
            "Epoch 88: Train Loss: 1.2876, Val Loss: 1.2847\n",
            "Epoch 89: Train Loss: 1.2826, Val Loss: 1.2846\n",
            "Epoch 90: Train Loss: 1.2954, Val Loss: 1.2927\n",
            "Epoch 91: Train Loss: 1.2851, Val Loss: 1.2951\n",
            "Epoch 92: Train Loss: 1.2819, Val Loss: 1.2871\n",
            "Epoch 93: Train Loss: 1.2842, Val Loss: 1.2840\n",
            "Epoch 94: Train Loss: 1.2847, Val Loss: 1.2842\n",
            "Epoch 95: Train Loss: 1.2840, Val Loss: 1.2766\n",
            "Epoch 96: Train Loss: 1.2863, Val Loss: 1.2911\n",
            "Epoch 97: Train Loss: 1.2847, Val Loss: 1.2912\n",
            "Epoch 98: Train Loss: 1.2815, Val Loss: 1.2909\n",
            "Epoch 99: Train Loss: 1.2952, Val Loss: 1.2810\n",
            "Epoch 100: Train Loss: 1.2811, Val Loss: 1.2833\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Set device\n",
        "device = \"cpu\"\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_tensor = batch[\"input_tensor\"].to(device)\n",
        "        target_tensor = batch[\"target_tensor\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(input_tensor, target_tensor)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output.permute(0, 2, 1), target_tensor)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Function to evaluate the model on validation data\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_tensor = batch[\"input_tensor\"].to(device)\n",
        "            target_tensor = batch[\"target_tensor\"].to(device)\n",
        "\n",
        "            output = model(input_tensor, target_tensor)\n",
        "            loss = criterion(output.permute(0, 2, 1), target_tensor)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Training parameters\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "\n",
        "# Move model to device\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    train_loss = train_epoch(model, dataloader, criterion, optimizer, device)\n",
        "    val_loss = evaluate(model, dataloader, criterion, device)\n",
        "    print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01678c83",
      "metadata": {
        "id": "01678c83",
        "outputId": "ad628797-d318-4787-aba6-ef96a870c251"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(15376, 256)\n",
              "    (lstm): LSTM(256, 256, batch_first=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(15376, 256)\n",
              "    (lstm): LSTM(256, 256, batch_first=True)\n",
              "    (fc): Linear(in_features=256, out_features=15376, bias=True)\n",
              "    (softmax): LogSoftmax(dim=1)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6de55d59",
      "metadata": {
        "id": "6de55d59"
      },
      "outputs": [],
      "source": [
        "def generate_summary(input_text, model, tokenizer, device, max_summary_length=50):\n",
        "    # Tokenize the input text\n",
        "    print(\"Input Text:\", input_text)\n",
        "    inputs = tokenizer([input_text], max_length=1024, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    print(\"Input Tensor:\", inputs)\n",
        "\n",
        "    # Generate the summary\n",
        "    with torch.no_grad():\n",
        "        target_seq = torch.zeros((inputs.input_ids.size(0), max_summary_length)).long().to(device)\n",
        "        output_ids = model(inputs.input_ids, target_seq=target_seq)\n",
        "\n",
        "    # Decode the generated summary\n",
        "    summary_ids = torch.argmax(output_ids, dim=-1)\n",
        "    print(\"Shape of summary_ids:\", summary_ids.shape)\n",
        "    print(\"Type of summary_ids:\", type(summary_ids))\n",
        "    print(\"Summary IDs:\", summary_ids)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    return summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ddcaa1d",
      "metadata": {
        "scrolled": false,
        "id": "5ddcaa1d",
        "outputId": "5b49756c-5765-48c2-8e3d-aa12673ca096"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Text: enter text here to get a summary. this model is trained to make a summary of the text you enter here.\n",
            "Input Tensor: {'input_ids': tensor([[    0, 11798,  2788,   259,     7,   120,    10,  4819,     4,    42,\n",
            "          1421,    16,  5389,     7,   146,    10,  4819,     9,     5,  2788,\n",
            "            47,  2914,   259,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1]])}\n",
            "Shape of summary_ids: torch.Size([1, 50])\n",
            "Type of summary_ids: <class 'torch.Tensor'>\n",
            "Summary IDs: tensor([[    0,   216,  1418,   792, 15375, 15375, 15375, 15374, 15374, 15374,\n",
            "         15374, 15374, 15374, 15374, 15374, 15374, 15374, 15374, 15374, 15374,\n",
            "         15374, 15374, 15374, 15374, 15374, 15374, 15374, 15374, 15374, 15374,\n",
            "         15374, 15374, 15374, 15374, 15374, 15374, 15374, 15374, 15374, 15374,\n",
            "         15374, 15374, 15374, 15374, 15374, 15374, 15374, 15374, 15374, 15374]])\n",
            "Generated Summary:  know district board���astingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingastingasting\n"
          ]
        }
      ],
      "source": [
        "from transformers import BartTokenizer\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Example usage\n",
        "device = \"cpu\"\n",
        "input_text = \"enter text here to get a summary. this model is trained to make a summary of the text you enter here.\"\n",
        "summary = generate_summary(input_text, model, tokenizer, device)\n",
        "print(\"Generated Summary:\", summary)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}